---
title: RAG Agent using Docling and Weaviate
description: An LLM chat-like question-answering system with RAG (Retrieval-Augmented Generation) to provide accurate answers from PDF documents. The system leverages Docling to parse and intelligently chunk PDF documents, Weaviate as a vector database to store vectorized chunks, and OpenAI for embeddings and text generation.
---

import { CodeFetcher } from '../../../components/CodeFetcher'

## Let's build a PDF RAG Agent with:

- **PDF Document Processing**: Efficiently parses and chunks PDF documents for analysis.
- **Vector Storage with Weaviate**: Stores and manages vectorized document chunks.
- **Docling for Advanced Parsing**: Utilizes Docling for intelligent PDF parsing and hybrid chunking.
- **OpenAI Integration**: Leverages OpenAI for creating embeddings and generating text.
- **RAG Pattern for Q&A**: Implements Retrieval-Augmented Generation for accurate question answering.

## The Steps

<Folder name="steps" defaultOpen>
  <File name="api-process-pdfs.step.ts" />
  <File name="api-query-rag.step.ts" />
  <File name="init-weaviate.step.ts" />
  <File name="load-weaviate.step.ts" />
  <File name="process-pdfs.step.py" />
</Folder>

<Tabs items={['api-process-pdfs', 'api-query-rag', 'init-weaviate', 'load-weaviate', 'process-pdfs']}>
  <CodeFetcher path="examples/rag-docling-weaviate-agent/steps/api-steps" tab="api-process-pdfs" value="api-process-pdfs" />
  <CodeFetcher path="examples/rag-docling-weaviate-agent/steps/api-steps" tab="api-query-rag" value="api-query-rag" />
  <CodeFetcher path="examples/rag-docling-weaviate-agent/steps/event-steps" tab="init-weaviate" value="init-weaviate" />
  <CodeFetcher path="examples/rag-docling-weaviate-agent/steps/event-steps" tab="load-weaviate" value="load-weaviate" />
  <CodeFetcher path="examples/rag-docling-weaviate-agent/steps/event-steps" tab="process-pdfs" value="process-pdfs" />
</Tabs>

## üöÄ Features

- **PDF document processing and chunking**: Efficiently parse and chunk PDF documents.
- **Vector storage using Weaviate**: Store and manage vectorized document chunks.
- **Docling for PDF parsing and hybrid chunking**: Uses Docling for advanced document chunking.
- **OpenAI integration for embeddings and text generation**: Leverage OpenAI for creating embeddings and generating text.
- **Question answering using RAG pattern**: Retrieval-Augmented Generation for accurate question answering.

## üìã Prerequisites

- Node.js v18 or later
- npm or pnpm
- API keys for:
  - [OpenAI](https://platform.openai.com/) (Embeddings and text generation)
  - [Weaviate](https://weaviate.io/) (Vector database)

## üõ†Ô∏è Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/MotiaDev/motia-examples
   cd examples/rag-docling-weaviate-agent
   ```

2. Install dependencies:
   ```bash
   pnpm install
   # or
   npm install
   ```

3. Configure environment variables:
   ```bash
   cp .env.example .env
   ```

   Update `.env` with your API keys:
   ```env
   # Required
   OPENAI_API_KEY=your-openai-api-key-here
   WEAVIATE_API_KEY=your-weaviate-api-key-here
   WEAVIATE_URL=your-weaviate-url-here
   ```

## üèóÔ∏è Architecture

![RAG Docling Weaviate Agent](../img/ai-deep-research-agent.png)

## üèóÔ∏è Technologies

- **TypeScript**
- **Python**
- **Docling**
- **Weaviate**
- **OpenAI**

## üö¶ API Endpoints

### Start Research

```
POST /research
Content-Type: application/json

{
  "query": "The research topic or question",
  "breadth": 4,  // Number of search queries to generate (1-10)
  "depth": 2     // Depth of research iterations (1-5)
}
```

Response:
```json
{
  "message": "Research process started",
  "requestId": "unique-trace-id"
}
```

### Check Research Status

```
GET /research/status?requestId=unique-trace-id
```

Response:
```json
{
  "message": "Research status retrieved successfully",
  "requestId": "unique-trace-id",
  "originalQuery": "The research topic or question",
  "status": "in-progress",
  "progress": {
    "currentDepth": 1,
    "totalDepth": 2,
    "percentComplete": 50
  },
  "reportAvailable": false
}
```

### Get Research Report

```
GET /research/report?requestId=unique-trace-id
```

Response:
```json
{
  "message": "Research report retrieved successfully",
  "report": {
    "title": "Research Report Title",
    "overview": "Executive summary...",
    "sections": [
      {
        "title": "Section Title",
        "content": "Section content..."
      }
    ],
    "keyTakeaways": [
      "Key takeaway 1",
      "Key takeaway 2"
    ],
    "sources": [
      {
        "title": "Source Title",
        "url": "Source URL"
      }
    ],
    "originalQuery": "The research topic or question",
    "metadata": {
      "depthUsed": 2,
      "completedAt": "2025-03-18T16:45:30Z"
    }
  },
  "requestId": "unique-trace-id"
}
```

## üèÉ‚Äç‚ôÇÔ∏è Running the Application

1. Start the development server:
   ```bash
   pnpm dev
   ```

2. Access the Motia Workbench:
   ```
   http://localhost:3000
   ```

3. Make a test request:
   ```bash
   curl --request POST \
   --url http://localhost:3000/research \
   --header 'Content-Type: application/json' \
   --data '{
      "query": "Advancements in renewable energy storage",
      "depth": 1,
      "breadth": 1
   }'
   ```
## üôè Acknowledgments

- [Motia Framework](https://motia.dev) for the event-driven workflow engine
- [OpenAI](https://platform.openai.com/) for AI analysis 
- [Firecrawl](https://www.firecrawl.dev/) for Web search and content extraction API